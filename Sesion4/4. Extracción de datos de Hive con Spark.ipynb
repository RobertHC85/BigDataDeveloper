{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24878e2c-c21c-4cae-9ef7-5c5d35489582",
   "metadata": {
    "id": "24878e2c-c21c-4cae-9ef7-5c5d35489582",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36c7fea-4530-45c4-891f-14ed7fabd9dc",
   "metadata": {
    "id": "e36c7fea-4530-45c4-891f-14ed7fabd9dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b41148b-e7ca-4abb-ae59-d6bd91549ee8",
   "metadata": {
    "id": "8b41148b-e7ca-4abb-ae59-d6bd91549ee8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "25/05/14 02:45:11 WARN Utils: Your hostname, codespaces-367222 resolves to a loopback address: 127.0.0.1; using 10.0.5.131 instead (on interface eth0)\n",
      "25/05/14 02:45:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 02:45:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/14 02:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/14 02:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/14 02:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .config(\"hive.metastore.uris\", \"thrift://172.27.1.8:9083\")\\\n",
    "      .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    "      .enableHiveSupport() \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f12631-2690-413c-bd75-124cb9f956d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|retail_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6c0ec4-3c14-464e-ac1f-fe6a82812cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|           customers|      false|\n",
      "|  default|customers_partiti...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN default\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0965ab-0f74-482b-9eef-cbe4beb4f8ae",
   "metadata": {
    "id": "7b0965ab-0f74-482b-9eef-cbe4beb4f8ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from retail_db.categories\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bdb92c-1c46-4c8e-ac7b-97ed6ad89286",
   "metadata": {
    "id": "a9bdb92c-1c46-4c8e-ac7b-97ed6ad89286",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 03:02:07 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/user/hadoop/customers: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: namenode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect * from default.customers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/dataframe.py:978\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    971\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    972\u001b[39m         message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    975\u001b[39m         },\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: java.net.UnknownHostException: namenode"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.customers\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d97d2-d296-4854-a528-b038099bf676",
   "metadata": {
    "id": "b36d97d2-d296-4854-a528-b038099bf676",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from retail_db.departments\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541c55e-ebb6-4207-a961-55dea82a98e4",
   "metadata": {
    "id": "0541c55e-ebb6-4207-a961-55dea82a98e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from retail_db.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fac0bf-151a-46a5-a835-4fd4e8c2a11b",
   "metadata": {
    "id": "58fac0bf-151a-46a5-a835-4fd4e8c2a11b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from retail_db.order_items\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b906c2-cac5-4430-ab38-9f616c08995e",
   "metadata": {
    "id": "a1b906c2-cac5-4430-ab38-9f616c08995e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from retail_db.products\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fe072-3dda-4eed-b530-c2d5384a3cb6",
   "metadata": {
    "id": "1f8fe072-3dda-4eed-b530-c2d5384a3cb6"
   },
   "source": [
    "### Enunciado 1\n",
    "Mostrar un Top 20 de clientes que mas productos compraron con sus respectivos montos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c8d1d-37c6-435f-854a-a387457e91bb",
   "metadata": {
    "id": "ad8c8d1d-37c6-435f-854a-a387457e91bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "    customer_id, customer_fname, customer_lname, customer_email, sum(order_item_quantity) as quantity_item_total, sum(order_item_subtotal)as total\n",
    "FROM\n",
    "    retail_db.customers as c\n",
    "INNER JOIN\n",
    "    retail_db.orders as o\n",
    "    ON c.customer_id = o.order_customer_id\n",
    "INNER JOIN\n",
    "    retail_db.order_items as oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE order_status <> 'CANCELED'\n",
    "GROUP BY customer_id, customer_fname, customer_lname, customer_email\n",
    "ORDER BY  total DESC\n",
    "LIMIT 20\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02422d7-99b2-4c87-a46b-92e4f82fe94c",
   "metadata": {
    "id": "a02422d7-99b2-4c87-a46b-92e4f82fe94c"
   },
   "source": [
    "### Enunciado 2\n",
    "Mostrar las categorías con el total de productos vendidos y los montos totales por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e927a68-6441-46f9-a3a1-f19fd5a85354",
   "metadata": {
    "id": "2e927a68-6441-46f9-a3a1-f19fd5a85354",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "    ca.category_name, sum(order_item_quantity) as item_quantity, cast(sum(order_item_subtotal) AS INT )as total\n",
    "FROM retail_db.order_items as oi\n",
    "INNER JOIN\n",
    "    retail_db.products as p\n",
    "    ON oi.order_item_product_id = p.product_id\n",
    "INNER JOIN\n",
    "    retail_db.categories as ca\n",
    "    ON p.product_category_id = ca.category_id\n",
    "GROUP BY ca.category_name;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d8ccf-1103-499d-aa05-73b0756efd08",
   "metadata": {
    "id": "bc3d8ccf-1103-499d-aa05-73b0756efd08"
   },
   "source": [
    "### Enunciado 3\n",
    "Mostrar la categoría más vendida por ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5d0ec-5bba-43a0-92c4-48ff032e0eaa",
   "metadata": {
    "id": "aba5d0ec-5bba-43a0-92c4-48ff032e0eaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    customer_city, category_name\n",
    "FROM (SELECT\n",
    "    customer_city, category_name, count(category_name) as quantity, DENSE_RANK () OVER (\n",
    "                PARTITION BY customer_city\n",
    "                ORDER BY count(category_name) DESC\n",
    "            ) rank\n",
    "    FROM\n",
    "        retail_db.customers as c\n",
    "    INNER JOIN\n",
    "        retail_db.orders as o\n",
    "        ON c.customer_id = o.order_customer_id\n",
    "    INNER JOIN\n",
    "        retail_db.order_items as oi\n",
    "        ON o.order_id = oi.order_item_order_id\n",
    "    INNER JOIN\n",
    "        retail_db.products as p\n",
    "        ON oi.order_item_product_id = p.product_id\n",
    "    INNER JOIN\n",
    "        retail_db.categories as ca\n",
    "        ON p.product_category_id = ca.category_id\n",
    "    GROUP BY customer_city, category_name\n",
    "    ) t\n",
    "WHERE rank = 1;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81df90c-fab3-44dc-a485-b063f806a9ef",
   "metadata": {
    "id": "b81df90c-fab3-44dc-a485-b063f806a9ef"
   },
   "source": [
    "### Enunciado 4\n",
    "Mostrar los 5 productos más vendidos por cada ciudad y el monto recaudado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985160ac-0c23-43be-87c9-45b3c2b5db00",
   "metadata": {
    "id": "985160ac-0c23-43be-87c9-45b3c2b5db00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    customer_city, product_name, quantity, total\n",
    "FROM (SELECT\n",
    "    customer_city, product_name,sum(order_item_quantity) as quantity,sum(order_item_subtotal) as total, DENSE_RANK () OVER (\n",
    "                PARTITION BY customer_city\n",
    "                ORDER BY sum(order_item_quantity) DESC\n",
    "            ) rank\n",
    "    FROM\n",
    "        retail_db.customers as c\n",
    "    INNER JOIN\n",
    "        retail_db.orders as o\n",
    "        ON c.customer_id = o.order_customer_id\n",
    "    INNER JOIN\n",
    "        retail_db.order_items as oi\n",
    "        ON o.order_id = oi.order_item_order_id\n",
    "    INNER JOIN\n",
    "        retail_db.products as p\n",
    "        ON oi.order_item_product_id = p.product_id\n",
    "    GROUP BY customer_city, product_name\n",
    "    ) t\n",
    "WHERE rank < 6\n",
    "ORDER BY quantity DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cbead-c7bc-45a1-8c04-34b6423fafd7",
   "metadata": {
    "id": "169cbead-c7bc-45a1-8c04-34b6423fafd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
